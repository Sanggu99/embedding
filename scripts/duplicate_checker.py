"""
ì´ë¯¸ì§€ ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í¬ë¦½íŠ¸
- SHA-256 í•´ì‹œë¡œ ì™„ì „ ë™ì¼í•œ ì´ë¯¸ì§€ íƒì§€
- Perceptual hash (pHash)ë¡œ ì‹œê°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ íƒì§€
- ì¤‘ë³µ ì´ë¯¸ì§€ë¥¼ ë°±ì—… í´ë”ë¡œ ì´ë™
"""

import os
import json
import hashlib
import shutil
from pathlib import Path
from collections import defaultdict
from datetime import datetime
from PIL import Image
import imagehash

class DuplicateChecker:
    def __init__(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.source_folders = self.config['source_folders']
        self.backup_dir = Path(self.config['backup_dir'])
        self.output_dir = Path(self.config['output_dir'])
        self.extensions = self.config['image_extensions']
        self.phash_threshold = self.config.get('perceptual_hash_threshold', 10)
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.exact_duplicates = defaultdict(list)
        self.similar_duplicates = defaultdict(list)
        self.sha256_map = {}
        self.phash_map = {}
    
    def calculate_sha256(self, file_path):
        """íŒŒì¼ì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def calculate_phash(self, file_path):
        """ì´ë¯¸ì§€ì˜ perceptual hash ê³„ì‚°"""
        try:
            img = Image.open(file_path)
            return str(imagehash.phash(img))
        except Exception as e:
            print(f"âš ï¸  pHash ê³„ì‚° ì‹¤íŒ¨: {file_path} - {e}")
            return None
    
    def find_all_images(self):
        """ëª¨ë“  ì†ŒìŠ¤ í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°"""
        image_files = []
        for folder in self.source_folders:
            folder_path = Path(folder)
            if not folder_path.exists():
                print(f"âš ï¸  í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder}")
                continue
            
            for ext in self.extensions:
                image_files.extend(folder_path.rglob(f"*{ext}"))
        
        return image_files
    
    def check_duplicates(self, image_files):
        """ì¤‘ë³µ ì´ë¯¸ì§€ ê²€ì‚¬"""
        print(f"\nğŸ” {len(image_files)}ê°œ ì´ë¯¸ì§€ ê²€ì‚¬ ì¤‘...")
        
        for idx, file_path in enumerate(image_files, 1):
            if idx % 50 == 0:
                print(f"   ì§„í–‰: {idx}/{len(image_files)}")
            
            # SHA-256 í•´ì‹œ ê³„ì‚° (ì™„ì „ ì¤‘ë³µ)
            sha256 = self.calculate_sha256(file_path)
            
            if sha256 in self.sha256_map:
                # ì™„ì „ ì¤‘ë³µ ë°œê²¬
                self.exact_duplicates[sha256].append(str(file_path))
            else:
                self.sha256_map[sha256] = str(file_path)
            
            # Perceptual hash ê³„ì‚° (ì‹œê°ì  ìœ ì‚¬ì„±)
            phash = self.calculate_phash(file_path)
            if phash:
                # ìœ ì‚¬í•œ í•´ì‹œ ì°¾ê¸°
                found_similar = False
                for existing_phash in self.phash_map.keys():
                    # Hamming distance ê³„ì‚°
                    distance = imagehash.hex_to_hash(phash) - imagehash.hex_to_hash(existing_phash)
                    if distance <= self.phash_threshold:
                        self.similar_duplicates[existing_phash].append(str(file_path))
                        found_similar = True
                        break
                
                if not found_similar:
                    self.phash_map[phash] = str(file_path)
    
    def backup_and_remove_duplicates(self):
        """ì¤‘ë³µ ì´ë¯¸ì§€ë¥¼ ë°±ì—… í´ë”ë¡œ ì´ë™"""
        moved_count = 0
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_subdir = self.backup_dir / f"duplicates_{timestamp}"
        backup_subdir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ“¦ ì¤‘ë³µ ì´ë¯¸ì§€ë¥¼ ë°±ì—… í´ë”ë¡œ ì´ë™ ì¤‘...")
        
        # ì™„ì „ ì¤‘ë³µ ì²˜ë¦¬
        for sha256, duplicate_files in self.exact_duplicates.items():
            # ì²« ë²ˆì§¸ íŒŒì¼ì€ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” ë°±ì—…
            for dup_file in duplicate_files:
                source = Path(dup_file)
                if source.exists():
                    # ë°±ì—… ë””ë ‰í† ë¦¬ì— ì›ë³¸ í´ë” êµ¬ì¡° ìœ ì§€
                    relative_path = source.relative_to(Path(self.source_folders[0]).parent)
                    dest = backup_subdir / relative_path
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.move(str(source), str(dest))
                    moved_count += 1
                    print(f"   âœ“ ì´ë™: {source.name}")
        
        return moved_count, backup_subdir
    
    def generate_report(self):
        """ì¤‘ë³µ ê²€ì‚¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_images_scanned": len(self.sha256_map) + sum(len(v) for v in self.exact_duplicates.values()),
            "exact_duplicates_count": sum(len(v) for v in self.exact_duplicates.values()),
            "similar_duplicates_count": sum(len(v) for v in self.similar_duplicates.values()),
            "exact_duplicates": {
                sha256: {
                    "original": self.sha256_map.get(sha256, "Unknown"),
                    "duplicates": files
                }
                for sha256, files in self.exact_duplicates.items()
            },
            "similar_duplicates": {
                phash: {
                    "original": self.phash_map.get(phash, "Unknown"),
                    "similar_images": files
                }
                for phash, files in self.similar_duplicates.items()
            }
        }
        
        report_path = self.output_dir / "duplicate_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report_path
    
    def run(self):
        """ì „ì²´ ì¤‘ë³µ ê²€ì‚¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸ” ì´ë¯¸ì§€ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘")
        print("=" * 60)
        
        # 1. ëª¨ë“  ì´ë¯¸ì§€ ì°¾ê¸°
        image_files = self.find_all_images()
        print(f"\nâœ“ ì´ {len(image_files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        
        # 2. ì¤‘ë³µ ê²€ì‚¬
        self.check_duplicates(image_files)
        
        # 3. ê²°ê³¼ ì¶œë ¥
        exact_count = sum(len(v) for v in self.exact_duplicates.values())
        similar_count = sum(len(v) for v in self.similar_duplicates.values())
        
        print(f"\nğŸ“Š ê²€ì‚¬ ê²°ê³¼:")
        print(f"   â€¢ ì™„ì „ ì¤‘ë³µ: {exact_count}ê°œ")
        print(f"   â€¢ ì‹œê°ì  ìœ ì‚¬: {similar_count}ê°œ")
        
        # 4. ì¤‘ë³µ íŒŒì¼ ë°±ì—… ë° ì‚­ì œ
        if exact_count > 0:
            moved_count, backup_dir = self.backup_and_remove_duplicates()
            print(f"\nâœ“ {moved_count}ê°œ íŒŒì¼ì„ ë°±ì—…ìœ¼ë¡œ ì´ë™")
            print(f"   ë°±ì—… ìœ„ì¹˜: {backup_dir}")
        
        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        report_path = self.generate_report()
        print(f"\nâœ“ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
        
        print("\n" + "=" * 60)
        print("âœ… ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ!")
        print("=" * 60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    config_path = Path(__file__).parent.parent / "config.json"
    
    if not config_path.exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("config.json íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
    
    # ì¤‘ë³µ ê²€ì‚¬ ì‹¤í–‰
    checker = DuplicateChecker(config_path)
    checker.run()

if __name__ == "__main__":
    main()
